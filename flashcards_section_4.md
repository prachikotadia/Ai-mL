# Section 4: Deep Learning & Neural Networks

## 151. Deep Learning
游릱 **What is deep learning?**

游릴 **Definition**
Deep learning is a type of machine learning that uses neural networks with many layers. It learns patterns directly from large amounts of data. It works especially well for images, text, and audio.

游릳 **How It Works / Example**
For image recognition, a deep network learns simple edges in early layers and complex shapes in later layers. It improves by comparing predictions to labels and updating weights. Over time, it can recognize objects like cars and dogs.

游릵 **Quick Tip**
Learning representations with depth.

---

## 152. Neural Network
游릱 **What is a neural network?**

游릴 **Definition**
A neural network is a model made of connected layers that transform input data into outputs. It learns by adjusting weights between connections. It can model complex relationships that simple models cannot.

游릳 **How It Works / Example**
For sentiment analysis, the network takes text features and produces a positive/negative score. During training, it updates weights to reduce errors. After training, it can score new reviews.

游릵 **Quick Tip**
Inspired by the human brain.

---

## 153. Neuron
游릱 **What is a neuron in a neural network?**

游릴 **Definition**
A neuron is a small compute unit that combines inputs and produces an output. It multiplies inputs by weights, adds a bias, and then applies an activation function. Many neurons together form a layer.

游릳 **How It Works / Example**
A neuron might take features like "age" and "income," weight them, and output a score. That score then goes through an activation like ReLU. This helps the network build useful transformations step-by-step.

游릵 **Quick Tip**
The atomic unit of intelligence.

---

## 154. Activation Function
游릱 **What is an activation function?**

游릴 **Definition**
An activation function adds non-linearity to a neural network. Without it, the network behaves like a linear model even with many layers. Common activations include ReLU, sigmoid, and tanh.

游릳 **How It Works / Example**
ReLU outputs max(0, x), which keeps positive values and drops negative ones. This helps networks learn complex patterns. For example, it helps a vision model detect features that only matter when "present."

游릵 **Quick Tip**
The spark that fires the neuron.

---

## 155. ReLU
游릱 **What is ReLU and why is it popular?**

游릴 **Definition**
ReLU (Rectified Linear Unit) is an activation function that outputs 0 for negative inputs and x for positive inputs. It is simple and trains fast. It often reduces vanishing gradient problems compared to sigmoid.

游릳 **How It Works / Example**
In a CNN for images, ReLU is applied after convolution layers. Negative activations become 0, making the network sparse and efficient. This usually helps training converge faster.

游릵 **Quick Tip**
If positive, pass; if negative, zero.

---

## 156. Sigmoid
游릱 **What is sigmoid activation used for?**

游릴 **Definition**
Sigmoid maps a number to a value between 0 and 1. It is often used for binary classification outputs. It can cause vanishing gradients in deep networks.

游릳 **How It Works / Example**
A spam model might output a single value after sigmoid, like 0.85 spam probability. You choose a threshold like 0.5 to classify spam. For hidden layers, modern networks often prefer ReLU-like activations.

游릵 **Quick Tip**
S-curve for probabilities.

---

## 157. Tanh
游릱 **What is tanh activation?**

游릴 **Definition**
tanh maps values to a range from -1 to 1. It is centered around zero, which can help some optimization. Like sigmoid, it can still suffer from vanishing gradients in deep networks.

游릳 **How It Works / Example**
Some older RNNs used tanh in hidden states. It keeps values bounded to avoid exploding outputs. Today, tanh is still used in some gated architectures like LSTMs.

游릵 **Quick Tip**
Zero-centered S-curve.

---

## 158. Feedforward Neural Network (MLP)
游릱 **What is a feedforward neural network (MLP)?**

游릴 **Definition**
A feedforward neural network sends information from input to output through layers, without loops. It is also called an MLP (multi-layer perceptron). It is common for tabular data and simple tasks.

游릳 **How It Works / Example**
For predicting a customer score, you pass features through a few dense layers. Each layer applies weights and activations. The last layer outputs the prediction, like churn probability.

游릵 **Quick Tip**
One-way street for data.

---

## 159. Hidden Layer
游릱 **What is a hidden layer?**

游릴 **Definition**
A hidden layer is a layer between the input and output layers. It learns intermediate representations of the data. More hidden layers can capture more complex patterns.

游릳 **How It Works / Example**
In image tasks, early hidden layers learn edges and textures. Later layers learn object parts like eyes or wheels. These learned representations help the final output layer classify the image.

游릵 **Quick Tip**
Where the magic happens.

---

## 160. Output Layer
游릱 **What is the output layer in a neural network?**

游릴 **Definition**
The output layer produces the final prediction. Its shape and activation depend on the task. For example, classification often uses softmax or sigmoid, and regression often uses a linear output.

游릳 **How It Works / Example**
For 10-class digit recognition, the output layer has 10 numbers. Softmax turns them into probabilities that sum to 1. The highest probability class becomes the prediction.

游릵 **Quick Tip**
The final verdict.

---

## 161. Softmax
游릱 **What is softmax and how is it used?**

游릴 **Definition**
Softmax converts a list of scores into probabilities that sum to 1. It is commonly used for multi-class classification. It makes the model output interpretable as class probabilities.

游릳 **How It Works / Example**
A model outputs scores for {cat, dog, bird}. Softmax turns these into probabilities like {0.1, 0.8, 0.1}. You choose the highest probability as the predicted class.

游릵 **Quick Tip**
Winner takes all (probability).

---

## 162. Backpropagation
游릱 **What is backpropagation in deep learning?**

游릴 **Definition**
Backpropagation computes how each weight contributed to the error. It uses the chain rule to pass gradients backward through layers. These gradients are used to update weights.

游릳 **How It Works / Example**
After predicting an image label, you compute a loss. Backprop finds gradients for the last layer first, then earlier layers. The optimizer uses these gradients to adjust weights and reduce future loss.

游릵 **Quick Tip**
Blame assignment for errors.

---

## 163. Vanishing Gradient
游릱 **What is vanishing gradient?**

游릴 **Definition**
Vanishing gradient happens when gradients become very small in earlier layers. This makes deep networks learn slowly or stop learning. It is common with sigmoid or tanh in deep stacks.

游릳 **How It Works / Example**
In a very deep network, gradients shrink as they move backward. Early layers barely update, so they do not learn useful features. Using ReLU, residual connections, or better initialization helps.

游릵 **Quick Tip**
Signal fades away.

---

## 164. Exploding Gradient
游릱 **What is exploding gradient?**

游릴 **Definition**
Exploding gradient happens when gradients become very large. This can cause unstable updates and training to diverge. It is common in RNNs and deep networks without safeguards.

游릳 **How It Works / Example**
A model's loss may suddenly become "nan" due to huge gradients. Gradient clipping limits gradient size to stabilize training. Lower learning rates can also help.

游릵 **Quick Tip**
Signal blows up.

---

## 165. Weight Initialization
游릱 **What is weight initialization in deep learning?**

游릴 **Definition**
Weight initialization is how weights are set before training. Good initialization keeps activations and gradients stable. It helps training converge faster and avoid instability.

游릳 **How It Works / Example**
He initialization is often used with ReLU networks. It sets weight scales based on layer size. This helps avoid vanishing or exploding signals as data moves through layers.

游릵 **Quick Tip**
Starting on the right foot.

---

## 166. Dropout
游릱 **What is dropout and why is it used?**

游릴 **Definition**
Dropout is a regularization method that randomly turns off some neurons during training. It helps prevent overfitting by making the network not rely on any single path. During inference, dropout is turned off.

游릳 **How It Works / Example**
In each training step, dropout might remove 20% of hidden units. The model learns to be robust even when some signals are missing. This often improves test performance.

游릵 **Quick Tip**
Learning with one hand tied.

---

## 167. Batch Normalization
游릱 **What is batch normalization?**

游릴 **Definition**
Batch normalization normalizes activations within a layer using batch statistics. It can speed up training and improve stability. It also provides some regularization effect.

游릳 **How It Works / Example**
After a dense or conv layer, batch norm rescales activations to a stable range. This reduces sensitivity to initialization and learning rate. Many CNNs use batch norm to train deeper networks.

游릵 **Quick Tip**
Keeping activations in check.

---

## 168. Layer Normalization
游릱 **What is layer normalization?**

游릴 **Definition**
Layer normalization normalizes activations across features within one example. It does not depend on batch size. It is widely used in transformers.

游릳 **How It Works / Example**
In a transformer block, layer norm stabilizes hidden states for each token. This helps training remain stable even with small batches. It is a key part of modern NLP models.

游릵 **Quick Tip**
Norm per sample.

---

## 169. Convolutional Neural Network (CNN)
游릱 **What is a convolutional neural network (CNN)?**

游릴 **Definition**
A CNN is a neural network designed for grid-like data such as images. It uses convolution layers to detect local patterns like edges. CNNs are efficient because they reuse the same filters across the image.

游릳 **How It Works / Example**
A CNN slides small filters over an image to produce feature maps. Early filters detect edges and textures. Later layers combine these into higher-level patterns like faces or objects.

游릵 **Quick Tip**
Vision specialist.

---

## 170. Convolution Operation
游릱 **What is a convolution operation in CNNs?**

游릴 **Definition**
Convolution applies a small filter across an input to extract patterns. The filter produces a feature map showing where that pattern appears. It reduces the number of parameters compared to fully connected layers.

游릳 **How It Works / Example**
A 3칑3 filter scans an image and outputs high values where an edge pattern matches. This helps detect edges regardless of where they are in the image. Multiple filters learn different patterns.

游릵 **Quick Tip**
Sliding window search.

---

## 171. Pooling
游릱 **What is pooling in CNNs?**

游릴 **Definition**
Pooling reduces the size of feature maps while keeping important information. It helps with speed and makes the model less sensitive to small shifts. Common types are max pooling and average pooling.

游릳 **How It Works / Example**
Max pooling takes the maximum value in a small window like 2칑2. This keeps the strongest signal and shrinks the feature map. It helps a CNN recognize objects even if they move slightly.

游릵 **Quick Tip**
Downsample and summarize.

---

## 172. Stride
游릱 **What is stride in a convolution layer?**

游릴 **Definition**
Stride is how far the filter moves each step during convolution. A larger stride reduces output size more quickly. It affects both computation and how much detail is kept.

游릳 **How It Works / Example**
With stride 1, the filter moves one pixel at a time and keeps more detail. With stride 2, it jumps two pixels and downsamples faster. This can speed up the network but may lose fine details.

游릵 **Quick Tip**
Step size for the filter.

---

## 173. Padding
游릱 **What is padding in a convolution layer?**

游릴 **Definition**
Padding adds extra pixels (usually zeros) around the input. It helps control output size and lets filters see edge pixels better. Without padding, outputs shrink quickly.

游릳 **How It Works / Example**
If you apply a 3칑3 filter without padding, the output becomes smaller than the input. Adding "same" padding keeps output size similar. This helps deeper CNNs preserve spatial information longer.

游릵 **Quick Tip**
Border buffer.

---

## 174. Recurrent Neural Network (RNN)
游릱 **What is a recurrent neural network (RNN)?**

游릴 **Definition**
An RNN is a neural network designed for sequential data like text or time series. It uses a hidden state to carry information from earlier steps. It can model order and context.

游릳 **How It Works / Example**
For sentence processing, an RNN reads words one by one. The hidden state stores information about previous words. The final state can be used to predict sentiment or the next word.

游릵 **Quick Tip**
Network with memory.

---

## 175. LSTM
游릱 **What is an LSTM and why is it used?**

游릴 **Definition**
An LSTM is a special RNN that handles long-term dependencies better. It uses gates to control what to remember and what to forget. This reduces vanishing gradient problems in sequences.

游릳 **How It Works / Example**
In language tasks, LSTMs can remember information from earlier in a sentence. For example, it can keep track of subject-verb agreement. The gates decide which information should stay in memory.

游릵 **Quick Tip**
RNN with long-term memory.

---

## 176. GRU
游릱 **What is a GRU?**

游릴 **Definition**
A GRU is a gated RNN similar to an LSTM but simpler. It uses fewer gates and parameters. It often trains faster while still handling longer context than a basic RNN.

游릳 **How It Works / Example**
For time series prediction, a GRU processes values step-by-step and updates its hidden state using gates. It decides how much past information to keep. This can improve predictions over simple RNNs.

游릵 **Quick Tip**
LSTM's younger sibling.

---

## 177. Sequence-to-Sequence (Seq2Seq)
游릱 **What is a sequence-to-sequence model?**

游릴 **Definition**
A sequence-to-sequence model maps an input sequence to an output sequence. It is used in translation, summarization, and chat systems. It often uses an encoder to read input and a decoder to generate output.

游릳 **How It Works / Example**
In translation, the encoder reads an English sentence and creates a representation. The decoder generates the French sentence token by token. Attention often helps the decoder focus on the right input words.

游릵 **Quick Tip**
Encoder-Decoder architecture.

---

## 178. Attention
游릱 **What is attention in neural networks?**

游릴 **Definition**
Attention helps a model focus on the most relevant parts of the input. It improves handling of long sequences. It is a key idea behind transformers.

游릳 **How It Works / Example**
When translating a sentence, attention lets the model look at specific input words while generating each output word. It assigns higher weights to related words. This improves accuracy compared to relying only on a single final encoder state.

游릵 **Quick Tip**
Focusing on what matters.

---

## 179. Transformer
游릱 **What is a transformer model in deep learning?**

游릴 **Definition**
A transformer is a neural network that uses attention instead of recurrence. It processes tokens in parallel, making training faster. It is the main architecture behind modern LLMs.

游릳 **How It Works / Example**
A transformer reads all tokens at once and computes attention between them. This lets it capture relationships like "this word refers to that earlier word." Models like BERT and GPT are transformer-based.

游릵 **Quick Tip**
Attention is all you need.

---

## 180. Embedding Layer
游릱 **What is an embedding layer?**

游릴 **Definition**
An embedding layer converts discrete items like words into dense vectors. These vectors represent meaning or similarity. Embeddings are learned during training.

游릳 **How It Works / Example**
Words like "king" and "queen" may end up with similar vectors. The model uses these vectors as inputs instead of one-hot vectors. This makes learning more efficient and captures relationships.

游릵 **Quick Tip**
Words to vectors.

---

## 181. Transfer Learning
游릱 **What is transfer learning in deep learning?**

游릴 **Definition**
Transfer learning uses a model trained on one task as a starting point for another task. It saves time and improves performance when data is limited. It is common in vision and NLP.

游릳 **How It Works / Example**
You take a pretrained ResNet and fine-tune it for your custom image classes. The early layers already know general visual patterns. You only need to adjust later layers for your specific task.

游릵 **Quick Tip**
Standing on the shoulders of giants.

---

## 182. Fine-Tuning
游릱 **What is fine-tuning in deep learning?**

游릴 **Definition**
Fine-tuning is continuing training on a pretrained model using your dataset. It adapts the model to a specific task or domain. It usually requires less data than training from scratch.

游릳 **How It Works / Example**
You start from a pretrained language model and train it on customer support data. The model learns your company's terms and style. Then it answers questions more accurately for your domain.

游릵 **Quick Tip**
Polishing a pretrained model.

---

## 183. Freezing Layers
游릱 **What is freezing layers during fine-tuning?**

游릴 **Definition**
Freezing layers means not updating certain weights during training. It keeps pretrained knowledge stable and reduces training cost. It can help when you have limited data.

游릳 **How It Works / Example**
You freeze early CNN layers that detect edges and textures. You only train the final layers for your new classes. This often prevents overfitting and speeds up training.

游릵 **Quick Tip**
Locking in prior knowledge.

---

## 184. Residual Connection
游릱 **What is a residual connection (skip connection)?**

游릴 **Definition**
A residual connection adds the input of a layer to its output. It helps gradients flow through deep networks. This makes very deep models easier to train.

游릳 **How It Works / Example**
Instead of learning a full mapping, a layer learns a small change (a "residual"). The output becomes input + residual. ResNets use this idea to train hundreds of layers.

游릵 **Quick Tip**
Fast lane for gradients.

---

## 185. Residual Block
游릱 **What is a residual block in ResNet?**

游릴 **Definition**
A residual block is a group of layers with a skip connection around them. It lets the block learn a residual update instead of a full transformation. This improves training stability in deep CNNs.

游릳 **How It Works / Example**
A residual block may have two conv layers, then adds the original input to the output. If the best change is "do nothing," the block can learn near-zero residual. This prevents deep networks from getting worse as they add layers.

游릵 **Quick Tip**
Building block of deep nets.

---

## 186. Loss Landscape
游릱 **What is a loss landscape and why does it matter?**

游릴 **Definition**
A loss landscape is how loss changes as model weights change. It can have valleys, flat areas, and steep cliffs. Understanding it helps explain why optimization can be hard.

游릳 **How It Works / Example**
If the landscape is very steep, large learning rates can overshoot good solutions. If it is very flat, training may be slow. Techniques like momentum and adaptive optimizers help move through the landscape.

游릵 **Quick Tip**
Topography of error.

---

## 187. Dead ReLU
游릱 **What is a "dead ReLU" problem?**

游릴 **Definition**
Dead ReLU happens when a ReLU neuron outputs 0 for most inputs and stops learning. This can occur if weights shift so inputs are always negative. Then gradients become zero for that neuron.

游릳 **How It Works / Example**
If a layer's weights update badly, many activations may become negative and get clamped to 0. Those neurons stop contributing. Using Leaky ReLU, better initialization, or smaller learning rates can reduce this.

游릵 **Quick Tip**
Neurons that fell asleep.

---

## 188. Leaky ReLU
游릱 **What is Leaky ReLU and why is it used?**

游릴 **Definition**
Leaky ReLU is like ReLU but allows a small negative slope for negative inputs. It helps avoid dead ReLUs. It keeps small gradients flowing even when inputs are negative.

游릳 **How It Works / Example**
Instead of outputting 0 for negative inputs, it outputs something like 0.01x. This gives the neuron a chance to recover during training. It can improve stability in some models.

游릵 **Quick Tip**
ReLU with a leak.

---

## 189. Gradient Checking
游릱 **What is gradient checking?**

游릴 **Definition**
Gradient checking verifies that computed gradients are correct. It compares backprop gradients to numerical approximations. It is mainly used for debugging custom implementations.

游릳 **How It Works / Example**
You slightly change a weight and measure how loss changes. This approximates the gradient. If it matches backprop's gradient closely, your implementation is likely correct.

游릵 **Quick Tip**
Double-checking the math.

---

## 190. Autoencoder
游릱 **What is an autoencoder?**

游릴 **Definition**
An autoencoder is a neural network that learns to compress and reconstruct data. It has an encoder that reduces dimension and a decoder that rebuilds the input. It is used for representation learning and anomaly detection.

游릳 **How It Works / Example**
For anomaly detection, you train an autoencoder on normal data. It learns to reconstruct normal patterns well. When it sees abnormal data, reconstruction error becomes high, signaling an anomaly.

游릵 **Quick Tip**
Compress and decompress.

---

## 191. Bottleneck Layer
游릱 **What is a bottleneck layer in an autoencoder?**

游릴 **Definition**
A bottleneck layer is the compressed representation in the middle of an autoencoder. It forces the model to keep only the most important information. This learned representation can be used as features.

游릳 **How It Works / Example**
If an image autoencoder compresses 784 pixels to a 32-d vector, that 32-d vector is the bottleneck. It captures key information like shape and structure. You can use it for clustering or downstream tasks.

游릵 **Quick Tip**
The pinch point.

---

## 192. Variational Autoencoder (VAE)
游릱 **What is a variational autoencoder (VAE)?**

游릴 **Definition**
A VAE is an autoencoder that learns a probabilistic latent space. It can generate new samples by sampling from that space. It is used for generative modeling.

游릳 **How It Works / Example**
Instead of outputting a single latent vector, the encoder outputs a mean and variance. You sample a latent vector and decode it into an image. This lets you generate new images similar to the training data.

游릵 **Quick Tip**
Probabilistic generator.

---

## 193. GAN (Generative Adversarial Network)
游릱 **What is a GAN (Generative Adversarial Network)?**

游릴 **Definition**
A GAN is a generative model with two networks: a generator and a discriminator. The generator creates fake samples and the discriminator tries to detect fakes. They train together and improve each other.

游릳 **How It Works / Example**
To generate faces, the generator produces face images from random noise. The discriminator compares fake faces to real faces and learns to tell them apart. The generator learns to produce more realistic faces to fool the discriminator.

游릵 **Quick Tip**
The forger vs the detective.

---

## 194. Mode Collapse
游릱 **What is mode collapse in GANs?**

游릴 **Definition**
Mode collapse happens when a GAN generator produces limited varieties of outputs. It "collapses" to a few patterns that fool the discriminator. This reduces diversity in generated samples.

游릳 **How It Works / Example**
A face GAN might generate many faces that look very similar. The discriminator is fooled, so the generator keeps repeating that style. Techniques like better loss functions or regularization can reduce mode collapse.

游릵 **Quick Tip**
One-trick pony.

---

## 195. Loss Challenge
游릱 **What is a loss function choice challenge in deep learning?**

游릴 **Definition**
Choosing the wrong loss can lead to poor training even with a good model. The loss must match the task and data properties. Some losses are more stable or robust than others.

游릳 **How It Works / Example**
For classification, using MSE instead of cross-entropy can train slowly or poorly. For imbalanced data, you may need weighted loss. Matching the loss to the goal leads to better learning behavior.

游릵 **Quick Tip**
Measure what matters.

---

## 196. Class Weighting
游릱 **What is class weighting in deep learning training?**

游릴 **Definition**
Class weighting gives more importance to rare classes in the loss. It helps the model not ignore minority classes. It is common in imbalanced classification problems.

游릳 **How It Works / Example**
In fraud detection, you assign a larger weight to fraud examples. Mistakes on fraud cost more in the loss. This pushes the model to improve recall for fraud cases.

游릵 **Quick Tip**
Balancing the scales.

---

## 197. Focal Loss
游릱 **What is a focal loss and why is it used?**

游릴 **Definition**
Focal loss is a loss function designed for imbalanced classification. It reduces the impact of easy examples and focuses learning on hard examples. It is popular in object detection.

游릳 **How It Works / Example**
If the model is already confident on many non-fraud cases, focal loss downweights those. It emphasizes rare or confusing fraud cases. This can improve performance on the minority class.

游릵 **Quick Tip**
Focusing on the hard parts.

---

## 198. Mixed Precision Training
游릱 **What is mixed precision training?**

游릴 **Definition**
Mixed precision training uses lower-precision numbers (like FP16) for faster computation and less memory. It keeps some parts in higher precision to stay stable. It is common for large deep learning models on GPUs.

游릳 **How It Works / Example**
You store many activations in FP16 to reduce memory. You keep a master copy of weights in FP32 to avoid numeric issues. This can speed training significantly without hurting accuracy.

游릵 **Quick Tip**
Faster checks, same balance.

---

## 199. Gradient Accumulation
游릱 **What is gradient accumulation?**

游릴 **Definition**
Gradient accumulation simulates a larger batch size by adding gradients across multiple small batches. It helps when GPU memory is limited. The optimizer updates weights only after accumulating enough gradients.

游릳 **How It Works / Example**
If you want batch size 256 but can only fit 64, you run 4 steps and sum gradients. Then you do one weight update. This matches the effect of training with a larger batch.

游릵 **Quick Tip**
Saving up for a big step.

---

## 200. Compute vs Accuracy Tradeoff
游릱 **What is "compute vs accuracy tradeoff" in deep learning?**

游릴 **Definition**
This tradeoff is balancing model performance with training and inference cost. Larger models often perform better but need more time and hardware. Practical systems choose models that meet latency and budget needs.

游릳 **How It Works / Example**
A very large CNN may improve accuracy by 1%, but it may be too slow for a mobile app. You might choose a smaller model or use quantization. The best choice depends on product constraints.

游릵 **Quick Tip**
Cost vs Performance.
