# Section 2: Model Training & Evaluation

## 51. Model Training
ðŸŸ¦ **What is model training in machine learning?**

ðŸŸ© **Definition**
Model training is the process of teaching a model using data. The model changes its internal parameters to reduce mistakes. Training continues until the model learns useful patterns.

ðŸŸ¨ **How It Works / Example**
For example, to predict house prices, the model guesses prices and compares them to real prices. The difference becomes an error signal. The model updates its parameters to make future guesses closer.

ðŸŸª **Quick Tip**
Think of it as "practice" for the model.

---

## 52. Epoch
ðŸŸ¦ **What is an epoch in training?**

ðŸŸ© **Definition**
An epoch is one full pass through the entire training dataset. Models often need many epochs to learn well. More epochs can help, but too many can cause overfitting.

ðŸŸ¨ **How It Works / Example**
If you have 10,000 training samples, one epoch means the model has seen all 10,000 once. After each epoch, you may check validation performance. If validation gets worse, you may stop early.

ðŸŸª **Quick Tip**
One lap around the track (dataset).

---

## 53. Batch
ðŸŸ¦ **What is a batch during training?**

ðŸŸ© **Definition**
A batch is a small subset of training data used for one update step. Using batches makes training faster and fits in memory. Batch size can affect speed and model quality.

ðŸŸ¨ **How It Works / Example**
If batch size is 32, the model processes 32 examples and then updates weights once. It repeats this many times until it finishes an epoch. Smaller batches are noisier but often generalize well.

ðŸŸª **Quick Tip**
Bite-sized chunks of data.

---

## 54. Mini-batch Gradient Descent
ðŸŸ¦ **What is mini-batch gradient descent?**

ðŸŸ© **Definition**
Mini-batch gradient descent updates model weights using small batches of data. It is a common balance between speed and stability. It is used in most deep learning training.

ðŸŸ¨ **How It Works / Example**
Instead of using all data at once, you use batches like 64 samples. You compute the gradient on that batch and update weights. Repeating this over many batches gradually reduces loss.

ðŸŸª **Quick Tip**
The middle ground between Stochastic (1 sample) and Batch (all samples) GD.

---

## 55. Loss Function
ðŸŸ¦ **What is the loss function?**

ðŸŸ© **Definition**
A loss function measures how wrong the modelâ€™s predictions are. Training tries to minimize this loss. Different tasks use different losses.

ðŸŸ¨ **How It Works / Example**
In classification, cross-entropy loss punishes confident wrong predictions more. The model updates weights to reduce this loss. Lower loss usually means better predictions.

ðŸŸª **Quick Tip**
Also called "Cost Function" or "Error Function".

---

## 56. Objective Function
ðŸŸ¦ **What is the objective of training a model?**

ðŸŸ© **Definition**
The objective is to find model parameters that perform well on new data. In practice, this means minimizing loss on training while keeping generalization strong. Good training balances fit and simplicity.

ðŸŸ¨ **How It Works / Example**
You train, then check validation metrics like F1 or AUC. If validation improves, the model is learning useful patterns. If only training improves, it may be memorizing noise.

ðŸŸª **Quick Tip**
Minimize Loss, Maximize Generalization.

---

## 57. Gradient Descent
ðŸŸ¦ **What is gradient descent?**

ðŸŸ© **Definition**
Gradient descent is an optimization method to reduce loss. It changes model weights in the direction that decreases the loss. It is widely used to train neural networks.

ðŸŸ¨ **How It Works / Example**
The model computes how each weight affects the loss (the gradient). Then it updates weights by a small step opposite the gradient. Repeating this many times reduces the loss.

ðŸŸª **Quick Tip**
Walking down a hill blindfolded.

---

## 58. Learning Rate
ðŸŸ¦ **What is a learning rate?**

ðŸŸ© **Definition**
The learning rate controls how big each weight update step is. If it is too high, training may jump around and fail. If it is too low, training may be very slow.

ðŸŸ¨ **How It Works / Example**
If your loss is not decreasing, the learning rate might be too large. If loss decreases very slowly, it might be too small. People often try values like 1e-3 or 1e-4 for deep learning.

ðŸŸª **Quick Tip**
The throttle of the training process.

---

## 59. Backpropagation
ðŸŸ¦ **What is backpropagation?**

ðŸŸ© **Definition**
Backpropagation is the process of computing gradients in a neural network. It sends error information from the output layer back through the network. This tells each weight how to change to reduce loss.

ðŸŸ¨ **How It Works / Example**
After predicting, the network compares prediction to the label and gets a loss. Backprop uses the chain rule to compute gradients for all layers. Then an optimizer updates the weights.

ðŸŸª **Quick Tip**
Reverse-engineering the error.

---

## 60. Optimizer
ðŸŸ¦ **What is an optimizer in training?**

ðŸŸ© **Definition**
An optimizer is the algorithm that updates model parameters to reduce loss. It uses gradients from backpropagation. Common optimizers include SGD and Adam.

ðŸŸ¨ **How It Works / Example**
After computing gradients, the optimizer applies update rules to weights. Adam adapts step sizes per weight based on past gradients. This often helps training converge faster than basic SGD.

ðŸŸª **Quick Tip**
The strategist guiding the descent.

---

## 61. Stochastic Gradient Descent (SGD)
ðŸŸ¦ **What is stochastic gradient descent (SGD)?**

ðŸŸ© **Definition**
SGD updates weights using one sample or a small batch at a time. It makes noisy but frequent updates. This noise can sometimes help generalization.

ðŸŸ¨ **How It Works / Example**
Instead of waiting to compute a gradient on the full dataset, SGD updates after each batch. The loss curve may look bumpy, but it often improves over time. Many models train well with SGD plus momentum.

ðŸŸª **Quick Tip**
Fast, noisy, and effective.

---

## 62. Momentum
ðŸŸ¦ **What is momentum in optimization?**

ðŸŸ© **Definition**
Momentum helps optimization move faster in the right direction. It uses past update directions to smooth and speed up learning. It reduces zig-zag movement in steep areas.

ðŸŸ¨ **How It Works / Example**
Imagine pushing a ball downhill: it gains speed and keeps moving forward. Momentum accumulates gradients over steps. This helps the optimizer escape small bumps and move steadily.

ðŸŸª **Quick Tip**
Physics for optimization.

---

## 63. Adam Optimizer
ðŸŸ¦ **What is Adam optimizer?**

ðŸŸ© **Definition**
Adam is an optimizer that adapts learning rates for each parameter. It combines ideas from momentum and adaptive scaling. It is a common default for deep learning.

ðŸŸ¨ **How It Works / Example**
Adam keeps track of moving averages of gradients and squared gradients. It uses these to choose step sizes per weight. This often makes training stable without heavy tuning.

ðŸŸª **Quick Tip**
Adaptive Moment Estimation.

---

## 64. Weight Initialization
ðŸŸ¦ **What is weight initialization?**

ðŸŸ© **Definition**
Weight initialization is how you set model weights before training starts. Good initialization helps training converge faster and avoid unstable behavior. Poor initialization can cause vanishing or exploding values.

ðŸŸ¨ **How It Works / Example**
For deep networks, methods like Xavier or He initialization set weight scales carefully. This keeps activations from shrinking or blowing up across layers. Then training can progress smoothly.

ðŸŸª **Quick Tip**
Start right to finish right.

---

## 65. Early Stopping
ðŸŸ¦ **What is early stopping?**

ðŸŸ© **Definition**
Early stopping is stopping training when validation performance stops improving. It helps prevent overfitting. It saves compute and often improves generalization.

ðŸŸ¨ **How It Works / Example**
You track validation loss after each epoch. If it does not improve for, say, 5 epochs, you stop training. You keep the model checkpoint from the best validation score.

ðŸŸª **Quick Tip**
Quit while you're ahead.

---

## 66. Checkpoint
ðŸŸ¦ **What is a checkpoint in model training?**

ðŸŸ© **Definition**
A checkpoint is a saved snapshot of model weights during training. It lets you resume training or roll back to the best model. Checkpoints are important for long training runs.

ðŸŸ¨ **How It Works / Example**
You might save a checkpoint every epoch or when validation improves. If training crashes, you reload the latest checkpoint. You can also deploy the "best validation" checkpoint.

ðŸŸª **Quick Tip**
Save game.

---

## 67. Model Evaluation
ðŸŸ¦ **What is model evaluation?**

ðŸŸ© **Definition**
Model evaluation is measuring how well a model performs. You use metrics that match the business or task goal. Evaluation should be done on data the model did not train on.

ðŸŸ¨ **How It Works / Example**
For a fraud model, you might evaluate using precision, recall, and ROC-AUC on a test set. If recall is too low, you may miss fraud cases. The evaluation helps decide if the model is ready to deploy.

ðŸŸª **Quick Tip**
The report card.

---

## 68. Validation Metric
ðŸŸ¦ **What is a validation metric?**

ðŸŸ© **Definition**
A validation metric is a score computed on validation data during training. It helps you pick hyperparameters and compare models. It should reflect what you care about in production.

ðŸŸ¨ **How It Works / Example**
If you care about finding rare events, you might use recall or F1. After each epoch, you compute the metric on validation data. You then choose the model with the best validation metric.

ðŸŸª **Quick Tip**
The compass during training.

---

## 69. Test Set Usage
ðŸŸ¦ **What is the test set used for?**

ðŸŸ© **Definition**
The test set is used for final evaluation of the chosen model. It should be untouched during training and tuning. It estimates real-world performance.

ðŸŸ¨ **How It Works / Example**
You try multiple model ideas using training and validation. Once you pick the final version, you evaluate once on the test set. This avoids "cheating" by tuning directly to test results.

ðŸŸª **Quick Tip**
The final exam. No peeking!

---

## 70. Cross-Entropy Loss
ðŸŸ¦ **What is cross-entropy loss used for?**

ðŸŸ© **Definition**
Cross-entropy loss is used for classification problems. It compares predicted probabilities to the true class label. It encourages high probability on the correct class.

ðŸŸ¨ **How It Works / Example**
If the true label is "dog," the model should output a high probability for "dog." Cross-entropy penalizes it if it gives higher probability to "cat." Training updates weights to reduce this penalty.

ðŸŸª **Quick Tip**
Standard loss for classification.

---

## 71. Mean Squared Error (MSE)
ðŸŸ¦ **What is mean squared error (MSE) used for?**

ðŸŸ© **Definition**
MSE is a common loss for regression tasks. It measures the average squared difference between predicted and true values. Squaring makes large errors count more.

ðŸŸ¨ **How It Works / Example**
If a true price is 300k and prediction is 320k, the error is 20k. MSE squares it, making 400M (in squared units). This pushes the model to reduce big mistakes.

ðŸŸª **Quick Tip**
Punishes large errors heavily.

---

## 72. Mean Absolute Error (MAE)
ðŸŸ¦ **What is mean absolute error (MAE)?**

ðŸŸ© **Definition**
MAE measures the average absolute difference between predictions and true values. It is easier to interpret because it stays in the same units as the target. It is less sensitive to outliers than MSE.

ðŸŸ¨ **How It Works / Example**
If your prediction errors are 5, 10, and 20 minutes, MAE is the average of those absolute values. A few very large errors do not dominate as much as in MSE. This can be useful when outliers are common.

ðŸŸª **Quick Tip**
Robust to outliers.

---

## 73. Confusion Matrix Usage
ðŸŸ¦ **What is a confusion matrix used for?**

ðŸŸ© **Definition**
A confusion matrix shows counts of correct and incorrect predictions by class. It breaks results into true positives, false positives, true negatives, and false negatives. It helps diagnose error types.

ðŸŸ¨ **How It Works / Example**
For a medical test, a false negative is a sick person predicted healthy. The confusion matrix shows how many such cases happen. This helps you decide if the model is safe enough.

ðŸŸª **Quick Tip**
The detailed breakdown of mistakes.

---

## 74. Classification Threshold
ðŸŸ¦ **How does threshold choice affect classification results?**

ðŸŸ© **Definition**
A threshold turns predicted probabilities into class decisions. Changing it shifts the tradeoff between precision and recall. The "best" threshold depends on the business cost of errors.

ðŸŸ¨ **How It Works / Example**
If you lower the threshold for fraud detection, you catch more fraud (higher recall) but flag more real users (lower precision). If you raise it, you reduce false alarms but miss more fraud. Teams pick a threshold based on the cost of each mistake.

ðŸŸª **Quick Tip**
Default is 0.5, but rarely optimal.

---

## 75. Calibration
ðŸŸ¦ **What is calibration in model evaluation?**

ðŸŸ© **Definition**
Calibration means predicted probabilities match real-world frequencies. If a model says "0.8," about 80% of those cases should be positive. Good calibration is important for decision-making.

ðŸŸ¨ **How It Works / Example**
If a churn model predicts 0.7 churn probability for many users, you expect about 70% of them to churn. If only 40% churn, the model is overconfident. You can improve calibration using methods like Platt scaling.

ðŸŸª **Quick Tip**
Trustworthiness of probabilities.

---

## 76. Learning Curve
ðŸŸ¦ **What is a learning curve?**

ðŸŸ© **Definition**
A learning curve shows performance versus training time or dataset size. It helps diagnose underfitting, overfitting, and whether more data will help. It is a useful debugging tool.

ðŸŸ¨ **How It Works / Example**
You plot training and validation loss across epochs. If training loss goes down but validation loss goes up, you may be overfitting. If both losses stay high, the model may be underfitting.

ðŸŸª **Quick Tip**
Visual health check for models.

---

## 77. Hyperparameter Tuning
ðŸŸ¦ **What is hyperparameter tuning?**

ðŸŸ© **Definition**
Hyperparameter tuning is searching for the best training settings. These include learning rate, batch size, and model depth. Good tuning can greatly improve performance.

ðŸŸ¨ **How It Works / Example**
You train the same model with different learning rates and compare validation metrics. You keep the best configuration. Tools like grid search or random search automate this.

ðŸŸª **Quick Tip**
Fine-tuning the knobs.

---

## 78. Grid Search
ðŸŸ¦ **What is grid search?**

ðŸŸ© **Definition**
Grid search tries every combination of chosen hyperparameter values. It is simple but can be slow. It works best when there are few hyperparameters.

ðŸŸ¨ **How It Works / Example**
You might try learning rates {1e-2, 1e-3} and batch sizes {32, 64}. Grid search trains models for all combinations. You select the best validation result.

ðŸŸª **Quick Tip**
Brute force search.

---

## 79. Random Search
ðŸŸ¦ **What is random search?**

ðŸŸ© **Definition**
Random search tries random combinations of hyperparameters. It often finds good settings faster than grid search. It is useful when there are many hyperparameters.

ðŸŸ¨ **How It Works / Example**
Instead of testing every combo, you sample 30 random settings. You train 30 models and pick the best validation score. This saves time when the search space is large.

ðŸŸª **Quick Tip**
Often beats Grid Search.

---

## 80. Bayesian Optimization
ðŸŸ¦ **What is Bayesian optimization for hyperparameters?**

ðŸŸ© **Definition**
Bayesian optimization chooses new hyperparameter trials based on past results. It tries to find good settings with fewer experiments. It is useful when training is expensive.

ðŸŸ¨ **How It Works / Example**
After a few trials, it learns which regions of the search space look promising. Then it tests settings that are likely to improve. This can beat random search when each run is costly.

ðŸŸª **Quick Tip**
Smart search.

---

## 81. K-Fold Cross-Validation
ðŸŸ¦ **What is k-fold cross-validation?**

ðŸŸ© **Definition**
K-fold cross-validation splits data into k parts and runs k training rounds. Each round uses one part for testing and the rest for training. It gives a more stable estimate than a single split.

ðŸŸ¨ **How It Works / Example**
With k=5, you train 5 times and test on a different fold each time. You average the 5 test scores. This helps when the dataset is small and results vary a lot.

ðŸŸª **Quick Tip**
Maximize data usage.

---

## 82. Stratified Splitting
ðŸŸ¦ **What is stratified splitting?**

ðŸŸ© **Definition**
Stratified splitting keeps class proportions similar across train/validation/test splits. It helps when classes are imbalanced. It makes evaluation more reliable.

ðŸŸ¨ **How It Works / Example**
If fraud is 1% of data, a random split might accidentally put too little fraud in validation. Stratification keeps around 1% fraud in each split. This makes metrics like recall more meaningful.

ðŸŸª **Quick Tip**
Keeping the mix consistent.

---

## 83. Holdout Set
ðŸŸ¦ **What is a holdout set?**

ðŸŸ© **Definition**
A holdout set is a reserved dataset not used during training. It provides an unbiased check of performance. The test set is a common example of a holdout set.

ðŸŸ¨ **How It Works / Example**
You keep a holdout set locked until the end. After you finish model selection, you evaluate on it once. This helps you avoid over-tuning to your validation data.

ðŸŸª **Quick Tip**
The vault.

---

## 84. Performance Metric
ðŸŸ¦ **What is a performance metric?**

ðŸŸ© **Definition**
A performance metric is a number that measures model quality. Different tasks need different metrics, like accuracy, AUC, or MAE. Picking the right metric is important for good decisions.

ðŸŸ¨ **How It Works / Example**
For imbalanced classification, accuracy can be misleading, so you may use F1 or AUC. For forecasting, you may use MAE or MAPE. The metric should match the real cost of errors.

ðŸŸª **Quick Tip**
You get what you optimize for.

---

## 85. Log Loss
ðŸŸ¦ **What is log loss?**

ðŸŸ© **Definition**
Log loss measures how good predicted probabilities are for classification. It penalizes confident wrong predictions strongly. Lower log loss means better probability estimates.

ðŸŸ¨ **How It Works / Example**
If the true label is 1 and the model predicts 0.99, log loss is small. If it predicts 0.01, log loss is very large. This pushes the model to be both correct and well-calibrated.

ðŸŸª **Quick Tip**
Confidence matters.

---

## 86. R-squared (RÂ²)
ðŸŸ¦ **What is R-squared in regression?**

ðŸŸ© **Definition**
R-squared measures how much of the target variation the model explains. It ranges from 0 to 1 in many cases, where higher is better. It can be misleading when used alone.

ðŸŸ¨ **How It Works / Example**
If R-squared is 0.8, the model explains about 80% of the variability in the target. But a high R-squared does not guarantee good predictions on new data. You still need validation and error metrics.

ðŸŸª **Quick Tip**
Coefficient of Determination.

---

## 87. Residual
ðŸŸ¦ **What is a residual in regression?**

ðŸŸ© **Definition**
A residual is the difference between the true value and the predicted value. Residuals show where the model is making errors. Studying residuals helps diagnose model problems.

ðŸŸ¨ **How It Works / Example**
If a true price is 300k and prediction is 280k, the residual is +20k. You can plot residuals to see patterns. If residuals grow with price, the model may be missing a feature or nonlinearity.

ðŸŸª **Quick Tip**
Error = Actual - Predicted.

---

## 88. Evaluation Bias
ðŸŸ¦ **What is a bias in evaluation data?**

ðŸŸ© **Definition**
Bias in evaluation data means the test set does not represent real-world conditions. This can make your evaluation misleading. A model may look good in tests but fail in production.

ðŸŸ¨ **How It Works / Example**
If your test set contains mostly easy examples, accuracy may be high. But real users may send harder examples. Fix this by building test data that matches production distributions.

ðŸŸª **Quick Tip**
Testing easy mode vs hard mode.

---

## 89. Data Drift
ðŸŸ¦ **What is data drift in model performance?**

ðŸŸ© **Definition**
Data drift happens when input data changes over time. This can reduce model accuracy after deployment. Drift is common in real systems.

ðŸŸ¨ **How It Works / Example**
A fraud model trained last year may fail if scammers change behavior. The feature distributions shift, so predictions become less reliable. Monitoring and retraining can reduce drift issues.

ðŸŸª **Quick Tip**
Input distribution has moved.

---

## 90. Concept Drift
ðŸŸ¦ **What is concept drift?**

ðŸŸ© **Definition**
Concept drift happens when the relationship between inputs and labels changes. Even if inputs look similar, the "rules" change. This makes the model's learned mapping outdated.

ðŸŸ¨ **How It Works / Example**
In spam detection, spammers change wording and tactics. The same features may no longer mean the same label. You detect concept drift by tracking performance and retraining with newer labels.

ðŸŸª **Quick Tip**
The ground truths have changed.

---

## 91. Imbalanced Data Eval
ðŸŸ¦ **What is an imbalanced dataset evaluation pitfall?**

ðŸŸ© **Definition**
With imbalanced data, some metrics hide poor performance on the rare class. Accuracy can look high even if the model misses most positives. You need metrics focused on the rare class.

ðŸŸ¨ **How It Works / Example**
If only 1% are fraud, predicting "not fraud" always gives 99% accuracy. But recall for fraud is 0%. Using precision/recall, PR-AUC, or cost-based metrics gives a clearer picture.

ðŸŸª **Quick Tip**
Beware the accuracy paradox.

---

## 92. PR-AUC
ðŸŸ¦ **What is PR-AUC and why is it used?**

ðŸŸ© **Definition**
PR-AUC is the area under the precision-recall curve. It is useful when positives are rare. It focuses on how well the model finds positives without too many false alarms.

ðŸŸ¨ **How It Works / Example**
In fraud, you care about catching fraud while not flagging too many real users. PR-AUC summarizes this tradeoff across thresholds. It is often more informative than ROC-AUC for rare positives.

ðŸŸª **Quick Tip**
Better than ROC for imbalanced data.

---

## 93. Fairness Metric
ðŸŸ¦ **What is a fairness metric in model evaluation?**

ðŸŸ© **Definition**
A fairness metric checks whether model performance differs across groups. It helps detect harmful bias. Fairness matters in high-impact areas like hiring and lending.

ðŸŸ¨ **How It Works / Example**
You might compare false positive rates across demographic groups. If one group is flagged much more often, the model may be unfair. You can adjust data, features, or thresholds to reduce gaps.

ðŸŸª **Quick Tip**
AI Ethics in practice.

---

## 94. Reproducible Training
ðŸŸ¦ **What is a reproducible training run?**

ðŸŸ© **Definition**
A reproducible run means you can repeat training and get the same results. It helps debugging and trust in experiments. It requires controlling randomness and logging settings.

ðŸŸ¨ **How It Works / Example**
You set random seeds, save code versions, and log hyperparameters. You also store the dataset version used. Then others can rerun training and confirm the results.

ðŸŸª **Quick Tip**
Science requires repeatability.

---

## 95. Seed
ðŸŸ¦ **What is a seed in machine learning experiments?**

ðŸŸ© **Definition**
A seed controls randomness in training, like data shuffling and initialization. Using the same seed helps get repeatable results. Different seeds can lead to slightly different outcomes.

ðŸŸ¨ **How It Works / Example**
If you train a neural network twice with different seeds, accuracy may differ a bit. With the same seed, results are more consistent. Teams often run multiple seeds and report average performance.

ðŸŸª **Quick Tip**
The key to deterministic randomness.

---

## 96. Gradient Clipping
ðŸŸ¦ **What is gradient clipping?**

ðŸŸ© **Definition**
Gradient clipping limits how large gradients can become during training. It prevents unstable updates and exploding gradients. It is common in training RNNs and large models.

ðŸŸ¨ **How It Works / Example**
If gradients become huge, the optimizer can take a massive step and break training. Clipping scales gradients down to a maximum value. This keeps updates stable and loss from blowing up.

ðŸŸª **Quick Tip**
Speed limit for weight updates.

---

## 97. Warmup Schedule
ðŸŸ¦ **What is a warmup schedule for learning rate?**

ðŸŸ© **Definition**
Warmup gradually increases the learning rate at the start of training. It helps stabilize early training, especially for large models. After warmup, the learning rate follows a normal schedule.

ðŸŸ¨ **How It Works / Example**
You might start at a very small learning rate and increase to the target over the first 1,000 steps. This avoids large early updates when weights are random. Transformers often use warmup for smoother training.

ðŸŸª **Quick Tip**
Earning the legs before running.

---

## 98. Learning Rate Decay
ðŸŸ¦ **What is learning rate decay?**

ðŸŸ© **Definition**
Learning rate decay reduces the learning rate over time. It helps the model make large progress early and fine-tune later. It can improve final performance.

ðŸŸ¨ **How It Works / Example**
Early in training, a higher learning rate helps find a good region quickly. Later, a smaller learning rate helps settle into a better solution. Common decay types include step decay and cosine decay.

ðŸŸª **Quick Tip**
Cooling down to settle in.

---

## 99. Train/Validation Mismatch
ðŸŸ¦ **What is "train/validation mismatch"?**

ðŸŸ© **Definition**
Train/validation mismatch happens when training data and validation data come from different distributions. This can make validation results hard to interpret. The model may fail in real usage if evaluation data is not realistic.

ðŸŸ¨ **How It Works / Example**
If you train on high-quality studio images but validate on blurry phone images, performance may drop sharply. The model did not learn patterns that handle real conditions. Fix by making training data closer to production data.

ðŸŸª **Quick Tip**
Train like you fight.

---

## 100. Ablation Study
ðŸŸ¦ **What is an ablation study in model evaluation?**

ðŸŸ© **Definition**
An ablation study tests how much each part of a system contributes. You remove or change one component at a time. This helps you understand what truly improves performance.

ðŸŸ¨ **How It Works / Example**
If you add a new feature and accuracy improves, you confirm by training without that feature. If performance drops, the feature mattered. You can ablate model layers, data sources, or preprocessing steps too.

ðŸŸª **Quick Tip**
Testing by removing parts.
